<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="A project homepage for Phi-Ground tech report">
  <meta property="og:title" content="Phi-Ground Tech Report" />
  <meta property="og:description" content="A project homepage for Phi-Ground tech report" />
  <meta property="og:url" content="" />
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/images/ico.png" />
  <meta property="og:image:width" content="1200" />
  <meta property="og:image:height" content="630" />


  <meta name="twitter:title" content="Phi-Ground Tech Report">
  <meta name="twitter:description" content="A project homepage for Phi-Ground tech report">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/ico.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="GUI grounding">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>Phi-Ground Tech Report</title>
  <link rel="icon" type="image/x-icon" href="static/favicon.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>

<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Phi-Ground Tech Report:  Advancing Perception in GUI Grounding</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="FIRST AUTHOR PERSONAL LINK" target="_blank">Miaosen Zhang</a>,</span>
              <span class="author-block">
                <a href="SECOND AUTHOR PERSONAL LINK" target="_blank">Ziqiang Xu</a>,</span>
              <span class="author-block">
                <a href="THIRD AUTHOR PERSONAL LINK" target="_blank">Jialiang Zhu</a>,</span>
              <span class="author-block">
                <a href="THIRD AUTHOR PERSONAL LINK" target="_blank">Qi Dai</a>,</span>
              <span class="author-block">
                <a href="THIRD AUTHOR PERSONAL LINK" target="_blank">Kai Qiu</a>,</span>
              <span class="author-block">
                <a href="THIRD AUTHOR PERSONAL LINK" target="_blank">Yifan Yang</a>,</span>
              <br>
              <span class="author-block">
                <a href="THIRD AUTHOR PERSONAL LINK" target="_blank">Chong Luo</a>,</span>
              <span class="author-block">
                <a href="THIRD AUTHOR PERSONAL LINK" target="_blank">Tianyi Chen</a>,</span>
              <span class="author-block">
                <a href="THIRD AUTHOR PERSONAL LINK" target="_blank">Justin Wagle</a>,</span>
              <span class="author-block">
                <a href="THIRD AUTHOR PERSONAL LINK" target="_blank">Tim Franklin</a>,</span>
              <span class="author-block">
                <a href="THIRD AUTHOR PERSONAL LINK" target="_blank">Baining Guo</a>,</span>
              
            </div>

            <div class="is-size-5 publication-authors">
              <span class="author-block"> Microsoft <br>August 2025</span>
              <!-- <span class="eql-cntrb"><small><br><sup>*</sup>Indicates Equal Contribution</small></span> -->
            </div>

            <div class="column has-text-centered">
              <div class="publication-links">
                <!-- Github link -->
                <span class="link-block">
                  <a href="https://github.com/zhangmiaosen2000/Phi-Ground" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>GitHub</span>
                  </a>
                </span>

                <!-- ArXiv abstract Link -->
                <span class="link-block">
                  <a href="https://arxiv.org/abs/<ARXIV PAPER ID>" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="ai ai-arxiv"></i>
                    </span>
                    <span>arXiv</span>
                  </a>
                </span>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>


  <!-- Teaser video-->
  <section class="hero teaser">
    <div class="container is-max-desktop">
      <div class="hero-body">
        <!-- <video poster="" id="tree" autoplay controls muted loop height="100%">
          <source src="static/videos/banner_video.mp4" type="video/mp4">
        </video> -->
        <img src="static/images/abstract.png" alt="" />
        <h2 class="subtitle has-text-centered">
          <b>Left:</b> The comparison chart of our grounding model results across five GUI grounding benchmarks. 
          Our model, trained specifically for the agent setting, achieved SOTA results on all benchmarks under this focus. 
          Even in the general end-to-end model setting, our model attained SOTA results on three of the benchmarks. 
          <b>Right:</b> The relationship between model performance and computational cost on ScreenSpot-pro demonstrates that 
          our model supports the Pareto frontier, indicating its efficiency. 
          Most GUI research traditionally considers only the parameter count N for comparison, 
          but our experiments highlight that computational cost during testing, such as the number of image tokens, 
          also significantly impacts performance. The X-axis in the right figure represents ND, where D is the number of image tokens. 
          Training and inference latency are more linearly correlated with ND than with N. 
          A graph using latency as the X-axis closely resembles the right figure, 
          but latency is often influenced by hardware and acceleration libraries such as vllm, so we did not use latency as X-axis.
        </h2>
      </div>
    </div>
  </section>
  <!-- End teaser video -->

  <!-- Paper abstract -->
  <section class="section hero is-light">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Abstract</h2>
          <div class="content has-text-justified">
            <p>
              With the development of multimodal reasoning models, Computer Use Agents (CUAs), akin to Jarvis from <i>"Iron Man"</i>, are becoming a reality. 
              GUI grounding is a crucial step for CUAs to perform concrete actions, as it determines the coordinates for clicks and other interactions. 
              Current end-to-end grounding models still achieve less than 80% accuracy on challenging benchmarks like ScreenSpot-pro and UI-Vision, 
              indicating they are far from being ready for deployment, as a single misclick can result in unacceptable consequences. 
              In this work, we conduct an empirical study on the training of grounding models, examining every detail from data collection to model training. 
              Ultimately, we developed the Phi-Ground model family, which achieves state-of-the-art performance across all five grounding benchmarks for models 
              under 10B parameters in agent settings. In the end-to-end model setting, our model still achieves SOTA results with scores of <i><b>43.2</b></i> 
              on ScreenSpot-pro and <i><b>27.2</b></i> on UI-Vision. We believe that the various details discussed in this paper, 
              along with our successes and failures, have generalization potential for other perception tasks. 
            </p>
          </div>
        </div>
      </div>
    </div>
  </section>
  <!-- End paper abstract -->


  <!-- Image carousel -->
  <section class="hero is-small">
    <div class="hero-body">
      <div class="container">
        <div id="results-carousel" class="carousel results-carousel">
          <div class="item">
            <!-- Your image here -->
            <img src="static/images/f1.png" alt="" />
            <h2 class="subtitle has-text-centered">
              <b>Agent evolution across physical and virtual worlds. </b>Traditional systems rely on fixed controllers and pre-defined workflows to execute domain-specific tasks, either in physical environments (e.g., task-specific robots) or virtual environments (e.g., API-based Web/APP agents). In the modern era, intelligent automation has emerged. In the physical world, general-purpose robots perform versatile limb-based operations. In the virtual world, Computer Use Agents (CUAs) achieve human-level behaviors through general purpose planner, GUI grounding, enabling them to complete any virtual task achievable via mouse and keyboard interactions.
            </h2>
          </div>
          <div class="item">
            <!-- Your image here -->
            <img src="static/images/f2.png" alt="" />
            <h2 class="subtitle has-text-centered">
              <b>Three levels of task of CUAs. Each coral block represent an action step.</b> CUA can be divided into two steps when completing a task: 
              <b>temporal planning</b> and <b>grounding</b>. 
              Planning involves analyzing the task description and the current state to determine the actions that should be taken in the future, w
              hile grounding refers to the execution of these specific actions. 
              In the context of Computer Use scenarios, grounding primarily involves generating computer-interactive commands, which include keyboard, 
              mouse instructions and etc. Since keyboard commands, such as pressing the "A" key, are discrete, MLLMs can effectively handle this type of grounding. 
              Our focus is primarily on mouse commands, where the main challenge lies in the fact that mouse command parameters are screen coordinates, 
              and most MLLMs struggle to accurately identify these coordinates. Therefore, specialized training is required for determining the precise click coordinates.
            </h2>
          </div>
          <div class="item">
            <!-- Your image here -->
            <img src="static/images/f3.png" alt="" />
            <h2 class="subtitle has-text-centered">
              <b>CommonCrawl data processing pipeline.</b>
            </h2>
          </div>
          <div class="item">
            <!-- Your image here -->
            <img src="static/images/f4.png" alt="" />
            <h2 class="subtitle has-text-centered">
              <b>Examples of benchmarks used in evaluation. </b> To ensure the model's generalization capability and to avoid systematic overfitting to well-known benchmarks such as ScreenSpot, we have gathered several recent open-source and internally developed evaluation datasets. This approach aims to ensure the comprehensiveness of our testing.
            </h2>
          </div>
          <div class="item">
            <!-- Your image here -->
            <img src="static/images/f5.png" alt="MY ALT TEXT" />
            <h2 class="subtitle has-text-centered">
              <b>Up: </b> Illustration of the impact of modal input order on model training. 
              <b>Down: </b> Comparison of input order of modalities.
            </h2>
          </div>
          <div class="item">
            <!-- Your image here -->
            <img src="static/images/f6.png" alt="MY ALT TEXT" />
          </div>
          <div class="item">
            <!-- Your image here -->
            <img src="static/images/f7.png" alt="MY ALT TEXT" />
          </div>
          <div class="item">
            <!-- Your image here -->
            <img src="static/images/f8.png" alt="MY ALT TEXT" />
            <h2 class="subtitle has-text-centered">
              Illustration of the evaluation results in relation to the training computation load. 
              The Y-axis represents the benchmark scores in click accuracy, while the X-axis denotes the training computation per sample in TFLOPs. 
              This training computation is estimated using the formula FLOPs = 6ND, where N is the number of image tokens and D is the number of model parameters.
            </h2>
          </div>
          <div class="item">
            <!-- Your image here -->
            <img src="static/images/f9.png" alt="MY ALT TEXT" />
            <h2 class="subtitle has-text-centered">
              Types and Proportions of Errors on the ScreenSpot-pro Benchmark. In each image, the red rectangles represent the regions corresponding to the ground truth. Red circles indicate erroneous outputs from the previous stage, while green circles denote correct outputs from the current stage. The centers of the green circles fall within the ground truth boundaries. To avoid obstructing the image content, we have enlarged the green circles in some of the images.
            </h2>
          </div>
        </div>
      </div>
    </div>
  </section>
  <!-- End image carousel -->





  <!-- Paper poster -->
  <section class="hero is-small is-light">
    <div class="hero-body">
      <div class="container">
        <h2 class="title">Key results</h2>
        <div id="results-carousel" class="carousel results-carousel">
          <div class="item">
            <!-- Your image here -->
            <img src="static/images/r1.png" alt="" />
            <h2 class="subtitle has-text-centered">
              The comparison of results across five GUI grounding test sets, which were tested by us, is presented.
            </h2>
          </div>
          <div class="item">
            <!-- Your image here -->
            <img src="static/images/r2.png" alt="" />
            <h2 class="subtitle has-text-centered">
              Detailed ScreenSpot-V2 results.
            </h2>
          </div>
          <div class="item">
            <!-- Your image here -->
            <img src="static/images/r3.png" alt="" />
            <h2 class="subtitle has-text-centered">
              Detailed ScreenSpot-Pro result
            </h2>
          </div>
          <div class="item">
            <!-- Your image here -->
            <img src="static/images/r4.png" alt="MY ALT TEXT" />
            <h2 class="subtitle has-text-centered">
              Results of UI-Vision.
            </h2>
          </div>
          <div class="item">
            <!-- Your image here -->
            <img src="static/images/r5.png" alt="MY ALT TEXT" />
            <h2 class="subtitle has-text-centered">
              Showdown-click-dev results. For the latency of the models we tested, we report the inference speed of the models accelerated using the vllm Python 
              library if supported, otherwise we report the latency using huggingface transformers, marked with 'hf'.
              For the settings with GPT-4O and O4-mini as planners, we directly added 2.5 seconds (aligned with the original benchmark) 
              and 8 seconds (our tested average level, which may be highly dependent on the endpoint) to the original model latency, respectively. 
              *: Results from the original GitHub repository.
            </h2>
          </div>
          <div class="item">
            <!-- Your image here -->
            <img src="static/images/r6.png" alt="MY ALT TEXT" />
            <h2 class="subtitle has-text-centered">
              Golden dataset evaluation results.
            </h2>
          </div>
        </div>
      </div>
    </div>
  </section>
  <!--End paper poster -->


  <!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>BibTex Code Here</code></pre>
    </div>
  </section>
  <!--End BibTex citation -->


  <footer class="footer">
    <div class="container">
      <div class="columns is-centered">
        <div class="column is-8">
          <div class="content">

            <p>
              This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template"
                target="_blank">Academic Project Page Template</a> which was adopted from the <a
                href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
              You are free to borrow the source code of this website, we just ask that you link back to this page in the
              footer. <br> This website is licensed under a <a rel="license"
                href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
                Commons Attribution-ShareAlike 4.0 International License</a>.
            </p>

          </div>
        </div>
      </div>
    </div>
  </footer>

  <!-- Statcounter tracking code -->

  <!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

  <!-- End of Statcounter Code -->

</body>

</html>
